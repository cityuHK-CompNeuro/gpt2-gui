# Amazon Review Generator via Fine-Tuned GPT2 and Pytorch
GUI based on Fine-Tuned GPT2 model to predict text and generate amazon reviews.
  
Open-Source Pre-Trained Large Language Models have created new opportunities for text prediction and generation. This repository utilizes Open AI's GPT2 model, PyTorch, and Hugging Face's API to simultaneously predict text on next-letter and next-word levels. Running on an Nvidia RTX 2060 graphics card, the text entry and prediction (up to about 10 words) are instantaneous. Paragraphs of text can be generated by the model in just a few seconds.
  
The model was fine-tuned to predict and generate text in the style of an Amazon product review. Pre-trained GPT2 was fine-tuned on three product categories in the Amazon Reviews Dataset: Music, Kitchen, and Pets. These model parameters have been made available along with this repository.
  
The front-end GUI was built via PySimpleGUI in Python, and allows the user to:
1. Input text for auto-completion
2. Generate entire "Amazon Reviews" from the input text
3. Load various models from local directories 
  

## Dataset
[Amazon Reviews 2018 dataset](https://jmcauley.ucsd.edu/data/amazon/)  

## Quick Start

1. Download GPT2 pre-trained model in Pytorch which huggingface/pytorch-pretrained-BERT already made!
```Windows CMD
$ git clone https://github.com/cityuHK-CompNeuro/gpt2-gui.git && chdir gpt-gui
# setup requirements
$ pip install -r requirements.txt
```

2. Run textbox_UI_trained.py
```
$ python textbox_UI_trained.py
```
3. Type into the text box as though writing an Amazon review for your favorite music album  
- Hint: start by typing the name of your favorite musician  
  

## Dependencies
pytorch
numpy
PySimpleGUI

Hugging Face transformers

```
$ pip install -r requirements.txt
```

# Example Functionality
![Individual Letter Prediction](/README_support/text_prediction.gif)  
  
<!-- ![Example Artist Prediction](/README_support/frank_z.PNG)   -->
  

# Design Thinking
What are the most powerful NLP models available to date? Large-Language Models and Transformer Architectures.
  
OpenAI's GPT models have made enormous advances in recent years, and are now able to generate text that is indistinguishable from human-written prose. Versions of these models are now available open-source, pre-trained, and packaged with accessible APIs. When building a high-performance text-prediction system, GPT should be the first model investigated.   

  
# Challenges
### Encountered During Development
1. Compiling and packaging GPT and PyTorch backend for distribution with pure Python
  
2. GPT2 model quickly overfit the fine-tuning dataset. 
* with less than 1 million reviews in the training set per product category, the model quickly adopted the writing style, specific phrases, and category topic.
- For example, after fine-tuning on the Pets dataset, the model will generate oddly specific stories about dog, cat, or turtle behaviors.
- GPT2 may create more generalizable text 'reviews' if certain training hyperparameters are explored, such as smaller learning rates

### Future Challenges


## Author

Jeremiah Palmerston

## Refernces
  
Training strategy Follows:
https://colab.research.google.com/drive/13dZVYEOMhXhkXWfvSMVM1TTtUDrT6Aeh

Pytorch  
  
Hugging Face  
  
[Open AI's Hugging Face Space](https://huggingface.co/docs/transformers/model_doc/gpt2)
    
**Better Language Models and Their Implications**

> Our model, called GPT-2 (a successor to [GPT](https://blog.openai.com/language-unsupervised/)), was trained simply to predict the next word in 40GB of Internet text. Due to our concerns about malicious applications of the technology, we are not releasing the trained model. As an experiment in responsible disclosure, we are instead releasing a much [smaller model](https://github.com/openai/gpt-2) for researchers to experiment with, as well as a [technical paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). from [openAI Blog](https://blog.openai.com/better-language-models/)

This repository is simple implementation GPT-2 about **text-generator** in **Pytorch** with **compress code**

- The original repertoire is [openai/gpt-2](https://github.com/openai/gpt-2). Also You can Read Paper about gpt-2, ["Language Models are Unsupervised Multitask Learners"](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf). To Understand more detail concept, I recommend papers about Transformer Model.
- Good implementation GPT-2 in Pytorch which I referred to, [huggingface/pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT), You can see more detail implementation in huggingface repository.

- Transformer(Self-Attention) Paper : [Attention Is All You Need(2017)](https://arxiv.org/abs/1706.03762)
- First OpenAi-GPT Paper : [Improving Language Understanding by Generative Pre-Training(2018)](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
- See [OpenAI Blog](https://blog.openai.com/better-language-models/) about GPT-2 and Paper


## License

- OpenAi/GPT2 follows MIT license, huggingface/pytorch-pretrained-BERT is Apache license. 
- MIT license provided herein


