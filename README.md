# Amazon Review Generator
### via Fine-Tuned GPT2 and Pytorch
I fine-tuned a pre-trained GPT2 model to predict text and generate amazon reviews. This model can instantaneously predict text on next-letter and next-word levels. I fine-tuned the model three times on three product categories from the Amazon Reviews Dataset: CDs and Vinyl (Music), Prime Pantry (Kitchen), and Pet Supplies. 
  
Open-Source Pre-Trained Large Language Models have created new opportunities for text prediction and generation. This repository utilizes Open AI's GPT2 model, PyTorch, and Hugging Face's API to power a simple user-interface for text prediction and generation. Running on an Nvidia RTX 2060 graphics card, the text entry and prediction (up to about 10 words) are instantaneous. Paragraphs of text can be generated by the model in just a few seconds.
  
The model was fine-tuned to predict and generate text in the style of an Amazon product review. Pre-trained GPT2 was fine-tuned on three product categories in the Amazon Reviews Dataset: Music, Kitchen, and Pets. These model parameters have been made available along with this repository.
  
The front-end GUI was built via PySimpleGUI in Python, and allows the user to:
1. Input text for auto-completion
2. Generate entire "Amazon Reviews" from the input text
3. Load fine-tuned GPT2 models from local directories 
    
## Dataset
[Amazon Reviews 2018 dataset](https://jmcauley.ucsd.edu/data/amazon/)  
  
Three product categories were used to fine-tune GPT2:
1. CDs and Vinyl (Music)
2. Prime Pantry (Kitchen)
3. Pet Supplies (Pets)
  
For each product category, I extracted the 'recommended' reviews, and used this subset for GPT training. The dataset indicated when a review received an upvote from an Amazon customer. These reviews are indicated with the 'vote' key in the json file, and these reviews are generally of higher quality.

I trained on the Music dataset first, then loaded the model parameters to train again on the Kitchen dataset, then re-loaded the parameters to train on the Pet dataset.  

This experiment showed that the model overfit the dataset each time, as the model generated specific examples regarding the product type from ambiguous user input.
  
Three models are made available for download and loading in the UI: Music, Kitchen, Pets.
  
## Quick Start

1. Download GPT2 pre-trained model in Pytorch which huggingface/pytorch-pretrained-BERT already made!
```Windows CMD
$ git clone https://github.com/cityuHK-CompNeuro/gpt2-gui.git  
$ chdir gpt-gui  
# setup requirements
$ pip install -r requirements.txt
```

2. Run textbox_UI_trained.py
```
$ python textbox_UI_trained.py
```
3. Type into the text box as though writing an Amazon review for your favorite music album  
- Hint: start by typing the name of your favorite musician  
  

## Dependencies
pytorch  
numpy  
PySimpleGUI  
Hugging Face transformers  
  
# Example Functionality
I created animated gifs to illustrate the UI function.  
Please wait for the gifs to load (or download them!) to see the UI interaction work in real-time  
  
![Individual Letter Prediction](/README_support/text_prediction.gif)  
  
<!-- ![Example Artist Prediction](/README_support/frank_z.PNG)   -->
  
![Review Generation](/README_support/My_gen.gif)  
  
<!-- ![Example Artist Prediction](/README_support/frank_z.PNG)   -->
  

# Design Thinking
Pre-Trained Large-Language Models with Transformer Architectures are the most powerful NLP tools to date. OpenAI's GPT models can generate text that is indistinguishable from human-written prose. This model can predict text on per-letter and per-word levels instantaneously. 

This model provides a smooth and useful user experience for predicting and generating text. 
  
# Challenges
### Encountered During Development
1. Compiling and packaging GPT and PyTorch backend for distribution with pure Python
  
2. GPT2 model quickly overfit the fine-tuning dataset. 
- with less than 1 million reviews in the training set per product category, the model quickly adopted the writing style, specific phrases, and category topic.
- For example, after fine-tuning on the Pets dataset, the model will generate oddly specific stories about dog, cat, or turtle behaviors.
- GPT2 may create more generalizable text 'reviews' if certain training hyperparameters are explored, such as smaller learning rates
  
### Future Challenges
1. Large compute power necessary for training and inference  
- For the current application, a standard Nvidia RTX 2060 (12gb) graphics card took approx. 8hrs to train the GPT2 head on 1 million reviews
- The same graphics card can run inference instantaneously, and generate approx. 250 words in a few seconds
- Although not overwhelming, these costs may be too high depending on the application
  
2. Time necessary to fine-tune the model to produce satisfactory text with the style of Amazon reviews, but without arbitrarily specific examples
- As mentioned in the 'Dataset' section, the model demonstrated overfitting behavior quite quickly. This led to oddly specific text generation, which is not desired in most cases. Hyperparameter fine-tuning or shadow prompt engineering may overcome this behavior, but would likely take time/manpower to complete  

## Author

Jeremiah Palmerston, PhD  

## References
  
### Code Example for Fine-Tuning GPT2 with Pytorch and Hugging Face
Training script follows this notebook:  
https://colab.research.google.com/drive/13dZVYEOMhXhkXWfvSMVM1TTtUDrT6Aeh

### Necessary Packages/API
[Pytorch](https://pytorch.org/)  
  
[Hugging Face Transformers](https://huggingface.co/docs/transformers/index)  
  
[Open AI's Hugging Face Space](https://huggingface.co/docs/transformers/model_doc/gpt2)
    
### Original GPT2 Paper from OpenAI
See [OpenAI Blog](https://blog.openai.com/better-language-models/) regarding GPT-2  

